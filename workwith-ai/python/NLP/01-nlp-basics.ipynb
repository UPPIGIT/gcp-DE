{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Topics\n",
    "#Corpus --paragraph\n",
    "#Document --sentence\n",
    "#vocabulary --word\n",
    "#Words are the basic unit of text processing and analysis in NLP.\n",
    "#A word is a single distinct meaningful element of speech or writing.\n",
    "\n",
    "#Tokenization ?\n",
    "#Tokenization is the process of breaking down text into smaller units, called tokens.\n",
    "#Tokens can be words, phrases, or symbols, depending on the context and the goals of the analysis.\n",
    "#Tokenization is a crucial step in NLP as it allows for the analysis of text at a granular level.\n",
    "##For example, in a sentence like \"The cat sat on the mat\", tokenization would break it down into the individual words: [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"].\n",
    "##Tokenization can also involve more complex structures, such as phrases or sentences, depending on the requirements of the analysis.\n",
    "##Tokenization is often the first step in NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
    "##Tokenization can be done using various methods, including rule-based approaches, machine learning models, or pre-built libraries.\n",
    "##Tokenization is essential for preparing text data for further processing and analysis in NLP applications.\n",
    "\n",
    "#Vocabulary ?\n",
    "#Vocabulary refers to the set of words that are known and used by an individual or a group.\n",
    "#In the context of NLP, vocabulary is the collection of unique words present in a corpus or dataset.\n",
    "#A vocabulary can be used to represent the words in a text in a structured way, allowing for easier analysis and processing.\n",
    "#A vocabulary can be built from a corpus by identifying all unique words and assigning them unique identifiers, such as indices or tokens.\n",
    "#The size of the vocabulary can vary depending on the corpus and the specific requirements of the NLP task.\n",
    "#A larger vocabulary can capture more nuances and variations in language, but it also increases the complexity of the analysis.\n",
    "#Vocabulary can be used in various NLP tasks, such as text classification, sentiment analysis, and machine translation.\n",
    "#In NLP, vocabulary is often represented as a mapping of words to their corresponding indices or tokens.\n",
    "#A vocabulary can also include additional information, such as word frequency or part-of-speech tags, to enhance the analysis.\n",
    "#Vocabulary is a fundamental concept in NLP, as it provides the foundation for understanding and processing text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is NLTK?\n",
    "NLTK is a powerful Python library for working with human language data (text). It provides tools for tasks like tokenization, stemming, lemmatization, part-of-speech tagging, and more, commonly used in natural language processing (NLP) tasks such as text analysis, sentiment analysis, or chatbots.\n",
    "\n",
    "Why use NLTK?\n",
    "\n",
    "Itâ€™s beginner-friendly and widely used for educational purposes.\n",
    "It supports a variety of NLP tasks with built-in datasets and pretrained models.\n",
    "Useful for processing and analyzing text data in applications like text mining or machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\upend\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\upend\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "Tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text into words.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tokens (words) from the input text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "# Example usage\n",
    "text = \"Hello, world! This is a sample text for tokenization.\"\n",
    "tokens = tokenize_text(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Hello, world! This is a sample text for tokenization.\n",
      "It contains multiple sentences, each with its own structure and meaning.\n",
      "This is the second sentence.\n",
      "And here is the third one, which is a bit longer than the others.\n",
      "This is the fourth sentence, which is even longer than the third one.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "corpus = \"\"\"Hello, world! This is a sample text for tokenization.\n",
    "It contains multiple sentences, each with its own structure and meaning.\n",
    "This is the second sentence.\n",
    "And here is the third one, which is a bit longer than the others.\n",
    "This is the fourth sentence, which is even longer than the third one.\"\"\"\n",
    "\n",
    "print(\"Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello, world!', 'This is a sample text for tokenization.', 'It contains multiple sentences, each with its own structure and meaning.', 'This is the second sentence.', 'And here is the third one, which is a bit longer than the others.', 'This is the fourth sentence, which is even longer than the third one.']\n",
      "Tokens from sentence: ['This', 'is', 'the', 'first', 'sentence', 'of', 'the', 'corpus', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def tokenize_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Tokenizes the input corpus into sentences.\n",
    "    \n",
    "    Parameters:\n",
    "    corpus (str): The input corpus to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of sentences from the input corpus.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    return sentences\n",
    "# Example usage\n",
    "sentences = tokenize_corpus(corpus)\n",
    "print(\"Sentences:\", sentences)\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes the input sentence into words.\n",
    "    \n",
    "    Parameters:\n",
    "    sentence (str): The input sentence to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tokens (words) from the input sentence.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return tokens\n",
    "# Example usage\n",
    "sentence = \"This is the first sentence of the corpus.\"\n",
    "tokens = tokenize_sentence(sentence)\n",
    "print(\"Tokens from sentence:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from sentence: ['Hello', ',', 'world', '!']\n",
      "Tokens from sentence: ['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']\n",
      "Tokens from sentence: ['It', 'contains', 'multiple', 'sentences', ',', 'each', 'with', 'its', 'own', 'structure', 'and', 'meaning', '.']\n",
      "Tokens from sentence: ['This', 'is', 'the', 'second', 'sentence', '.']\n",
      "Tokens from sentence: ['And', 'here', 'is', 'the', 'third', 'one', ',', 'which', 'is', 'a', 'bit', 'longer', 'than', 'the', 'others', '.']\n",
      "Tokens from sentence: ['This', 'is', 'the', 'fourth', 'sentence', ',', 'which', 'is', 'even', 'longer', 'than', 'the', 'third', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in sentences:\n",
    "    tokens = tokenize_sentence(sentences)\n",
    "    print(\"Tokens from sentence:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'world',\n",
       " '!',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'for',\n",
       " 'tokenization',\n",
       " '.',\n",
       " 'It',\n",
       " 'contains',\n",
       " 'multiple',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'each',\n",
       " 'with',\n",
       " 'its',\n",
       " 'own',\n",
       " 'structure',\n",
       " 'and',\n",
       " 'meaning',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'And',\n",
       " 'here',\n",
       " 'is',\n",
       " 'the',\n",
       " 'third',\n",
       " 'one',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'the',\n",
       " 'others',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "corpus= \"\"\"Hello, world! This is a sample text for tokenization.\n",
    "It contains multiple sentences, each with its own structure and meaning.\n",
    "This is the second sentence.\n",
    "And here is the third one, which is a bit longer than the others.\"\"\"\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello, world!', 'This is a sample text for tokenization.', 'It contains multiple sentences, each with its own structure and meaning.']\n",
      "Tokens from sentence: ['Hello', ',', 'world', '!']\n",
      "Tokens from sentence: ['This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']\n",
      "Tokens from sentence: ['It', 'contains', 'multiple', 'sentences', ',', 'each', 'with', 'its', 'own', 'structure', 'and', 'meaning', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello, world! This is a sample text for tokenization. It contains multiple sentences, each with its own structure and meaning.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(\"Tokens from sentence:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPunct Tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#word puck tokenization\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "text = \"Hello, world! This is a sample text for tokenization.\"\n",
    "tokens = wordpunct_tokenize(text)\n",
    "print(\"WordPunct Tokens:\", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'sample', 'text', 'for', 'tokenization.', 'It', 'contains', 'multiple', 'sentences', ',', 'each', 'with', 'its', 'own', 'structure', 'and', 'meaning', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Hello, world! This is a sample text for tokenization. It contains multiple sentences, each with its own structure and meaning.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Treebank Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_tokenize: ['NLTK is amazing!', \"It can't process Dr. Smith's data.\"]\n",
      "word_tokenize: ['NLTK', 'is', 'amazing', '!', 'It', 'ca', \"n't\", 'process', 'Dr.', 'Smith', \"'s\", 'data', '.']\n",
      "wordpunct_tokenize: ['NLTK', 'is', 'amazing', '!', 'It', 'can', \"'\", 't', 'process', 'Dr', '.', 'Smith', \"'\", 's', 'data', '.']\n",
      "TreebankWordTokenizer: ['NLTK', 'is', 'amazing', '!', 'It', 'ca', \"n't\", 'process', 'Dr.', 'Smith', \"'s\", 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "text = \"NLTK is amazing! It can't process Dr. Smith's data.\"\n",
    "\n",
    "# sent_tokenize\n",
    "sentences = sent_tokenize(text) \n",
    "print(\"sent_tokenize:\", sentences)\n",
    "\n",
    "# word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"word_tokenize:\", words)\n",
    "\n",
    "# wordpunct_tokenize\n",
    "words_punct = wordpunct_tokenize(text)\n",
    "print(\"wordpunct_tokenize:\", words_punct)\n",
    "\n",
    "# TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words_treebank = tokenizer.tokenize(text)\n",
    "print(\"TreebankWordTokenizer:\", words_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['NLTK is amazing!', \"It can't process Dr. Smith's data.\", \"Let's see how it handles different tokenization methods.\"]\n",
      "Words: ['NLTK', 'is', 'amazing', '!', 'It', 'ca', \"n't\", 'process', 'Dr.', 'Smith', \"'s\", 'data', '.', 'Let', \"'s\", 'see', 'how', 'it', 'handles', 'different', 'tokenization', 'methods', '.']\n",
      "WordPunct Tokens: ['NLTK', 'is', 'amazing', '!', 'It', 'can', \"'\", 't', 'process', 'Dr', '.', 'Smith', \"'\", 's', 'data', '.', 'Let', \"'\", 's', 'see', 'how', 'it', 'handles', 'different', 'tokenization', 'methods', '.']\n",
      "Treebank Tokens: ['NLTK', 'is', 'amazing', '!', 'It', 'ca', \"n't\", 'process', 'Dr.', 'Smith', \"'s\", 'data.', 'Let', \"'s\", 'see', 'how', 'it', 'handles', 'different', 'tokenization', 'methods', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "text = \"NLTK is amazing! It can't process Dr. Smith's data. Let's see how it handles different tokenization methods.\"\n",
    "# sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "# word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"Words:\", words)\n",
    "# wordpunct_tokenize\n",
    "words_punct = wordpunct_tokenize(text)\n",
    "print(\"WordPunct Tokens:\", words_punct)\n",
    "# TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words_treebank = tokenizer.tokenize(text)\n",
    "print(\"Treebank Tokens:\", words_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_tokenize: ['I love NLP!', \"It's super fun, isn't it?\", \"Mr. Jones' data-analysis rocks.\"]\n",
      "word_tokenize: ['I', 'love', 'NLP', '!', 'It', \"'s\", 'super', 'fun', ',', 'is', \"n't\", 'it', '?', 'Mr.', 'Jones', \"'\", 'data-analysis', 'rocks', '.']\n",
      "wordpunct_tokenize: ['I', 'love', 'NLP', '!', 'It', \"'\", 's', 'super', 'fun', ',', 'isn', \"'\", 't', 'it', '?', 'Mr', '.', 'Jones', \"'\", 'data', '-', 'analysis', 'rocks', '.']\n",
      "TreebankWordTokenizer: ['I', 'love', 'NLP', '!', 'It', \"'s\", 'super', 'fun', ',', 'is', \"n't\", 'it', '?', 'Mr.', 'Jones', \"'\", 'data-analysis', 'rocks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "# Download required NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I love NLP! It's super fun, isn't it? Mr. Jones' data-analysis rocks.\"\n",
    "\n",
    "# 1. sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"sent_tokenize:\", sentences)\n",
    "\n",
    "# 2. word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"word_tokenize:\", words)\n",
    "\n",
    "# 3. wordpunct_tokenize\n",
    "words_punct = wordpunct_tokenize(text)\n",
    "print(\"wordpunct_tokenize:\", words_punct)\n",
    "\n",
    "# 4. TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words_treebank = tokenizer.tokenize(text)\n",
    "print(\"TreebankWordTokenizer:\", words_treebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'ran', 'runner', 'happili', 'happi', 'happi', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "#what is stemming ?\n",
    "# Stemming is the process of reducing words to their base or root form.\n",
    "# It involves removing suffixes or prefixes from words to obtain their stem.\n",
    "# For example, the words \"running\", \"ran\", and \"runner\" can all be reduced to the stem \"run\".\n",
    "# Stemming is often used in NLP tasks to reduce the dimensionality of text data and improve the performance of models.\n",
    "\n",
    "words = [\"running\", \"ran\", \"runner\", \"happily\", \"happiness\", \"happy\", \"happier\", \"happiest\"]\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming the words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'am', 'running', 'and', 'jumping', '.', 'She', 'runs', 'and', 'jumps', 'daily', '.']\n",
      "Stemmed words: ['i', 'am', 'run', 'and', 'jump', '.', 'she', 'run', 'and', 'jump', 'daili', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am running and jumping. She runs and jumps daily.\"\n",
    "\n",
    "# Step 1: Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Step 2: Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step 3: Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words (Porter): ['run', 'ran', 'runner', 'happili', 'happi', 'happi', 'happier', 'happiest']\n",
      "Stemmed words (Regexp): ['runn', 'ran', 'runner', 'happi', 'happiness', 'happy', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, RegexpStemmer\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Initialize the Regexp Stemmer with a custom pattern\n",
    "regexp_stemmer = RegexpStemmer('ing$|ly$|ed$')\n",
    "# Sample words\n",
    "words = [\"running\", \"ran\", \"runner\", \"happily\", \"happiness\", \"happy\", \"happier\", \"happiest\"]\n",
    "# Apply stemming using Porter Stemmer\n",
    "stemmed_words_porter = [stemmer.stem(word) for word in words]\n",
    "# Apply stemming using Regexp Stemmer\n",
    "stemmed_words_regexp = [regexp_stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words (Porter):\", stemmed_words_porter)\n",
    "print(\"Stemmed words (Regexp):\", stemmed_words_regexp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['Processed', 'data', 'and', 'processing', 'tasks', 'are', 'completed', '.']\n",
      "Stemmed words: ['Process', 'data', 'and', 'process', 'tasks', 'are', 'complet', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample technical text\n",
    "text = \"Processed data and processing tasks are completed.\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Initialize RegexpStemmer for -ed and -ing\n",
    "stemmer = RegexpStemmer('ed$|ing$', min=5)\n",
    "\n",
    "# Stem words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Tokenized words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words (Snowball): ['run', 'ran', 'runner', 'happili', 'happi', 'happi', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "# Initialize the Snowball Stemmer for English\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# Sample words\n",
    "words = [\"running\", \"ran\", \"runner\", \"happily\", \"happiness\", \"happy\", \"happier\", \"happiest\"]\n",
    "# Apply stemming using Snowball Stemmer\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words (Snowball):\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'am', 'running', 'and', 'jumping', '.', 'She', 'runs', 'and', 'jumps', 'happily', '.']\n",
      "Stemmed words: ['i', 'am', 'run', 'and', 'jump', '.', 'she', 'run', 'and', 'jump', 'happili', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download required NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"I am running and jumping. She runs and jumps happily.\"\n",
    "\n",
    "# Step 1: Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Step 2: Initialize SnowballStemmer for English\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Step 3: Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['running', 'jumps', 'workable', 'happiness', 'connecting']\n",
      "Word           Porter         Snowball       Lancaster      Regexp         \n",
      "running        run            run            run            runn           \n",
      "jumps          jump           jump           jump           jump           \n",
      "workable       workabl        workabl        work           work           \n",
      "happiness      happi          happi          happy          happines       \n",
      "connecting     connect        connect        connect        connect        \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"running jumps workable happiness connecting\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "lancaster = LancasterStemmer()\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "# Apply stemming\n",
    "porter_stemmed = [porter.stem(word) for word in words]\n",
    "snowball_stemmed = [snowball.stem(word) for word in words]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in words]\n",
    "regexp_stemmed = [regexp.stem(word) for word in words]\n",
    "\n",
    "# Print results\n",
    "print(\"{0:15}{1:15}{2:15}{3:15}{4:15}\".format(\"Word\", \"Porter\", \"Snowball\", \"Lancaster\", \"Regexp\"))\n",
    "for i, word in enumerate(words):\n",
    "    print(\"{0:15}{1:15}{2:15}{3:15}{4:15}\".format(\n",
    "        word, porter_stemmed[i], snowball_stemmed[i], lancaster_stemmed[i], regexp_stemmed[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization ?\n",
    "# Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma.\n",
    "# It involves looking up the word in a dictionary to find its base form, which is often more accurate than stemming.\n",
    "# For example, the words \"running\", \"ran\", and \"runner\" can all be reduced to the lemma \"run\".\n",
    "# Lemmatization is often used in NLP tasks to improve the accuracy of text analysis and processing.\n",
    "# Lemmatization is particularly useful in tasks such as information retrieval, text classification, and sentiment analysis, where understanding the meaning of words is crucial.\n",
    "# Unlike stemming, which may produce non-words or incorrect forms, lemmatization ensures that the resulting words are valid and meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['running', 'ran', 'runner', 'happily', 'happiness', 'happy', 'happier', 'happiest']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Sample words\n",
    "words = [\"running\", \"ran\", \"runner\", \"happily\", \"happiness\", \"happy\", \"happier\", \"happiest\"]\n",
    "# Apply lemmatization to each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'am', 'running', 'and', 'jumping', '.', 'She', 'runs', 'and', 'jumps', 'happily', '.']\n",
      "Lemmatized words (default, noun POS): ['I', 'am', 'running', 'and', 'jumping', '.', 'She', 'run', 'and', 'jump', 'happily', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"I am running and jumping. She runs and jumps happily.\"\n",
    "\n",
    "# Step 1: Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Step 2: Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 3: Apply lemmatization (default: assumes noun POS)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words (default, noun POS):\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words (verb POS): ['I', 'be', 'run', 'and', 'jump', '.', 'She', 'run', 'and', 'jump', 'happily', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply lemmatization with verb POS\n",
    "lemmatized_words_verbs = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(\"Lemmatized words (verb POS):\", lemmatized_words_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'am', 'running', 'and', 'jumping', '.', 'She', 'runs', 'and', 'jumps', 'happily', '.', 'Better', 'solutions', 'exist', '.']\n",
      "POS tags: [('I', 'PRP'), ('am', 'VBP'), ('running', 'VBG'), ('and', 'CC'), ('jumping', 'NN'), ('.', '.'), ('She', 'PRP'), ('runs', 'VBZ'), ('and', 'CC'), ('jumps', 'NNS'), ('happily', 'RB'), ('.', '.'), ('Better', 'NNP'), ('solutions', 'NNS'), ('exist', 'VBP'), ('.', '.')]\n",
      "Lemmatized words (with POS): ['I', 'be', 'run', 'and', 'jumping', '.', 'She', 'run', 'and', 'jump', 'happily', '.', 'Better', 'solution', 'exist', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Helper function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ  # Adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB  # Verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN  # Noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV   # Adverb\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Sample text\n",
    "text = \"I am running and jumping. She runs and jumps happily. Better solutions exist.\"\n",
    "\n",
    "# Step 1: Tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Step 2: POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(\"POS tags:\", pos_tags)\n",
    "\n",
    "# Step 3: Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 4: Lemmatize with POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "print(\"Lemmatized words (with POS):\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['running', 'jumps', 'better', 'happiness', 'connecting']\n",
      "Word           Porter         Snowball       Lancaster      Regexp         Lemmatizer     \n",
      "running        run            run            run            runn           run            \n",
      "jumps          jump           jump           jump           jump           jump           \n",
      "better         better         better         bet            better         well           \n",
      "happiness      happi          happi          happy          happines       happiness      \n",
      "connecting     connect        connect        connect        connect        connect        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\upend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Helper function for WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Sample text\n",
    "text = \"running jumps better happiness connecting\"\n",
    "\n",
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# POS tagging for lemmatization\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Initialize stemmers and lemmatizer\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "lancaster = LancasterStemmer()\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "porter_stemmed = [porter.stem(word) for word in words]\n",
    "snowball_stemmed = [snowball.stem(word) for word in words]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in words]\n",
    "regexp_stemmed = [regexp.stem(word) for word in words]\n",
    "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "# Print results\n",
    "print(\"{0:15}{1:15}{2:15}{3:15}{4:15}{5:15}\".format(\"Word\", \"Porter\", \"Snowball\", \"Lancaster\", \"Regexp\", \"Lemmatizer\"))\n",
    "for i, word in enumerate(words):\n",
    "    print(\"{0:15}{1:15}{2:15}{3:15}{4:15}{5:15}\".format(\n",
    "        word, porter_stemmed[i], snowball_stemmed[i], lancaster_stemmed[i], regexp_stemmed[i], lemmatized[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'product', 'is', 'better', 'and', 'runs', 'smoothly', '.', 'Running', 'fast', 'is', 'great', '.']\n",
      "POS tags: [('The', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('better', 'RBR'), ('and', 'CC'), ('runs', 'VBZ'), ('smoothly', 'RB'), ('.', '.'), ('Running', 'VBG'), ('fast', 'RB'), ('is', 'VBZ'), ('great', 'JJ'), ('.', '.')]\n",
      "Lemmatized words: ['The', 'product', 'be', 'well', 'and', 'run', 'smoothly', '.', 'Running', 'fast', 'be', 'great', '.']\n",
      "Lemma frequencies: Counter({'be': 2, '.': 2, 'The': 1, 'product': 1, 'well': 1, 'and': 1, 'run': 1, 'smoothly': 1, 'Running': 1, 'fast': 1, 'great': 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# Helper function for WordNet POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Sample review\n",
    "text = \"The product is better and runs smoothly. Running fast is great.\"\n",
    "\n",
    "# Tokenize and POS tag\n",
    "words = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "\n",
    "# Count lemma frequencies\n",
    "lemma_counts = Counter(lemmatized)\n",
    "# Print results\n",
    "print(\"Original words:\", words)\n",
    "print(\"POS tags:\", pos_tags)\n",
    "print(\"Lemmatized words:\", lemmatized)\n",
    "print(\"Lemma frequencies:\", lemma_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
