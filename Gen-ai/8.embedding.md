## Embedding

Hello everyone and welcome back.

**Let's talk about embeddings now, which is a very interesting concept.So guys so far we saw multiple topics about generative AI.I am pretty sure that you went into ChatGPT and tried some questions and saw the response that you got.
Now all of that was great and perfect, but if you take a pause and think about it, there is something very fundamental that we haven't thought about.And that is machines do not understand text.They only understand numbers.**

---
# **So how do these llms understand what a word mean?**

**Or how can machines recognize the similarity between two text like when you ask it what is the capital of India? And it generates the word New Delhi?
That is where embeddings come into picture.**

---

**So an embedding is a numerical representation of text.And once you put words into numbers meaning once you generate these embeddings, it becomes very easy**

**for an to understand it and work with it.**


**So let's suppose that we have a text I eat ice cream.Now we need to convert this into a number so that the LM can understand it.It can generate more text around it or it can answer questions around it.So can we convert it into say, the number 20.Well that would not work.Why?Because we need to store more information.Just the meaning would not help.We also need to store other things like say, the context.**

---
```
Why? Because think about it.

Somebody says ice cream tastes great and
then somebody else says, my ice cream melted.Great.

These are two different things.

Even though they have the same word great.

But the same embedding would not work because you have different context in both the sentences.

The top one is a figure of happiness, but the bottom one is sarcasm.

So the embeddings need to convey just not the meaning of a sentence or a word, but also the context,the relationship, all of these things.

So how does this happen in reality?

Again, when we have a text like this, the first thing that the LM would do is break it into small chunks or token.

So the sentence now becomes four words or four tokens.

I eat ice and finally the word cream.

That's the first step that we call as chunking the words.

Then comes our neural network.And this is where the technology play happens.

Like we discussed, this neural network or transformer has been trained on millions and billions of words or token.

And in all of those trainings it has figured out how to accurately generate these embeddings.

And by accurate I mean generate embeddings in such a way that you have the meaning, you have the context,you have the relation, everything.

So this neural network would generate some random numbers like these.And these numbers are what are called as the embeddings.

Now here I am showing you very few embeddings for each word, but in reality there could be many, many
numbers or many, many embeddings generated for each word.

Now, what these embeddings mean is known only to the transformer model that generated them.This is what the training of the model has taught it how to generate these embeddings correctly.

How to retain the meaning.The relation context.

All of this information is stored in these embeddings and only the understands it.It uses it to generate output.

For example, based on the embeddings, the LM knows that the word I is followed by an action word like eat.It also knows that the words ice and cream go in together.That's why you see the embeddings for ice and cream are very similar to each other.This is how it was trained.And remember it has done this for billions of word.It has generated trillions of these embeddings, and that has been the training of a model like like ChatGPT.

And during this training, it has mastered the art of capturing all this information accurately in the embeddings.

So now when somebody asks the LM to generate some text, it would use all this learning that it has

got it would use all these embeddings that it has figured out, and it would use it to predict what word comes next.

It knows that if somebody is talking about eat ice, the next word would be cream.That's how it generates this word.

It will reference the embeddings.It would find out which ones are the closest match to what you already have, and keep predicting the next word one at a time.

Like I said earlier, also, when you are on like ChatGPT and you see it generating sentences, in reality

it is using all of its learning and using all of these embeddings to generate one word at a time to predict.One next word.

And that's how it would generate whole sentences, paragraphs, everything.It looks magical, but now you know it is these embeddings which are giving the that capability.

All right.

I hope that you are clear on embeddings.

I will see you in the next lecture.

Thank you. 
```